# Data management plan for TPChange

# 1. Administrative data
|   |   |
|---|---|
|Funding Agency	 |    DFG|
|Grant Reference Number	|SFB/TRR 301|
|Project Name	|The Tropopause in a changing atmopshere|
|Project Description	| See <https://tpchange.de> |
|PIs	|  See <https://tpchange.de>|
|Project Data Contact	|Daniel Kunkel  (<dkunkel@uni-mainz.de>)|
|Date of First Version 	|01.07.2022 |
|Date of Last Update	|11.06.2025 |
|Related Policies	| [DFG Leitlinie zum Umgang mit Forschungsdaten](https://www.dfg.de/download/pdf/foerderung/antragstellung/forschungsdaten/richtlinien_forschungsdaten.pdf)|
| |[DFG Leitlinien zur Sicherung guter wissenschaftlicher Praxis](https://www.dfg.de/download/pdf/foerderung/rechtliche_rahmenbedingungen/gute_wissenschaftliche_praxis/kodex_gwp.pdf)|

# 2. Data collection

## 2.1 Sources

TPChange is project at the interface of experimental and theoretical meteoreology. Data within the project covers products from laboratory and field experiments as well as model output of varying complexity. Third-party data from reanalysis data as well as observational data is essential in various proejcts within TPChange. 

Data also comprises program code which is used or which is created which is as essential as the data itself and thus requires to be versionized and eventually published in case of being used for publications. 

To clarify the terminology of data within this plan, the following differentiation is made:

| Type | Description |
|------|-------------|
|External data| Such data is created by third-parties and is available to be used in TPChange. Data sources are known and documented. If possible this data is centrally collected to be easily available for the members of TPChange and to ensure preservation in case the data is used for publications.  |
|Internal data| This data is created by members of TPChange within the projects of TPChange. Such data can comprise output from model experiments as well as results from laboratory and field experiments. In particular, the data from field experiements are crucial data, since they can not easily be reproduced. |
|External code| Program source code provided by third-parties, e.g. code from models such as ICON or MESSy. A second source of such code is proprietary code associated with technical equipment, e.g., for conversion of a measurement signal to atmospherically relevant quantities. A third source of external code is code retrieved from publically available repositories, e.g., GitHub which is used to process data of any kind. |
|Internal code| Source code which is created within the projects of TPChange, independently whether it is model, processing or analysis code.|

## 2.2 Formats

The interchangeability and easy usage of data is achieved through common data types and
formats as well as a comprehensible data structure. Data is preferably stored in the self-
describing format [netCDF](https://www.unidata.ucar.edu/software/netcdf/) where possible since netCDF is well-established within the meteorological community and supported by numerous software tools. Interfaces exist in particular for programming language such as Fortran and C/C++/C# as well as for interpreted languages such as python and julia. These interfaces are open sources and allow to read and write data which in turn allows for long-term accessibility.

Alternatively, text files like software code or data from laboratory and field experiemnts should be used and encoded using UTF8 to ensure portability. Text files from model code and model setups as well as results should be version controlled to make analyses transparent and allow repetition of experiments. 

## 2.3 Structure 

### 2.3.1 Centrally stored data
Within TPChange there are external and internal data sets which are hosted centrally by the projects Z1, Z2, and Z3. This includes data from the field campaign (see project Z01), reanalysis data  (Z02) and model input data (Z03). More so, data is included in this list which is of importance to several projects, e.g., data from the IAGOS database. In detail, centrally stored data is available as follows:

Centrally stored data is listed in the document: TRR301_central_data_sets.md 

The centrally stored data will be hosted by the associated contact person. This person is also responsible for meta data availability and potential backup/storage of the data, in particular in case the data is classified as internal data and can not be reproduced from external sources.

**Note:** data will be available on Mogon as well as on the TPChange server. The detailed data availability plan needs still to be implemented.


### 2.3.2 Project specific data
Within the individual projects other internal and external data will be created or used. This data should be described in the porject specific data management plan. More so, the project members are responsible to create meta-data information, versionize data if possible, and store essential data for eventual re-creation of the data.

Project specific data are listed in the document: TRR301_project_data_sets.md

### 2.3.3 Data availability 

Centrally stored data will be available on MOGON as well as on the TPChange server. The data on the TPChange server is required to be stored in iRODS to allow for quick recovery. The reanalysis data with its more than 200 TB of data is not stored in iRODS but can be recovered from its original source.

## 2.4 Data life cycle

A central element of the practical data management is the software iRODS. 
It provides the possibility to attach metadata to datasets, makes datasets searchable by metadata (e.g., key-words), 
provides a simplified interface to local tape archives, and enables data-exchange between the participating sites. 
To work with data, project scientists are provided with a per-project working directory, which can be organized as outlined above. 
Data within this working directory are not considered final and is used for ongoing analysis and modifications. Once a dataset or experiment is finalized (i.e., it is ready for publication or internal sharing), it is archived along with metadata. A typical workflow for a model experiment may contain the following steps:

1. Collection of required input and evaluation data. The iRODS client may be used to find the data in our archive. Datasets from the archive may only be linked into the working directory and not copied (unless they are only available on tape).
2. Quality control of retrieved data.
3. Perform the actual numerical experiment. This will create one or more new datasets within the working directory (e.g., model output, plots, etc.)
4. Create appropriate metadata and documentation. A third person should be able to understand how the experiment can be repeated. The usage of combined code and documentation (e.g., in format of Jupyter Notebooks6 or similar technologies) is encouraged but no strict requirement.
5. Archive the data using iRODS. This is usually done together with a publication of the scientific results. iRODS will transfer the data to the tape archive of the local computing center.
6. If feasible, publish the data and related code following the FAIR principles (Findable, Accessible, Interoperable, Reproducible). DFG guidelines for good scientific practice are applied (guideline 13, see related policies).
7. Delete data from the working directory when it is not needed for subsequent experiments or other projects.

## 2.5 Version control of source code

A central aspect of the data management is to provide a version control software. Such a tool allows to track and provides control over changes to any type of source code which will be created and or used within each project. We will use the state-of-the-art version control system [Git](https://git-scm.org) to establish a central code repository. Git will be used due to its distributed character, its ease of use and the fact that a web-based graphical user interface using Gitlab is already hosted at the Data Centre of the [JGU](https://gitlab.rlp.net). Our Gitlab will be used to share externally developed software, such as the weather model ICON (see also project Z03) as well as all internally developed software code or programme libraries. A common software repository will significantly increase the credibility and reproducibility of scientific results and simplifies re-use of individual members’ developments. Source code can then also be made available to the public using code sharing platforms, e.g., [Github](https://github.com/). The outcome of TPChange will be made available in a group, which further allows to gain maximum visibility. Additionally, persistent identifiers (e.g., digital object identifier - DOI) can be added to the repositories using OpenData services like Zenodo or comparable platforms. 

Zenodo is a publicly available data centre hosted at CERN which provides a DOI along with stored data, thus
making the data citable. More so, the data is directly accessible after publication and also version controlled.
Generally, users, i.e., project partners within the CRC will be supported through training courses on how to best
use Git as well as on documenting and publishing code. The support will be provided through on-site and digital
classes as well as through a communication platform which is linked to the Gitlab instance of the ZDV, also
allowing for interactive digital meetings using BigBlueButton. The ZDV already provides support for Gitlab on
mattermost. Training courses will be held specifically for project members in the first months of the CRC.


# 3. Data documentation and metadata

## 3.1  Data documentation
Metadata and data documentation are essential when storing data. A self-describing data format will already contain the most important metadata. Furthermore, within the iRODS data archive additional metadata is defined to comprehensively document stored data sets. For data sets from model simulations or laboratory and field experiments the owner is required to add additional information in human readable files, e.g., read-me files in markdown format. The collected information should contain everything needed to repeat any kind of experiments from the initial setup to the final output
following the DFG guidelines on good scientific practice.

## 3.2 Mandatory metadata
The most important metadata are collected within the actual data files using a self-describing format. The following metadata are required when datasets are stored within the iRODS archive:  

|Attribute |	Description |
| ---------| -------------- |
|Entry Name| 	A name that ensures unique identification. The combination of Project + Experiment + Dataset-Group must be unique, individual parts may not be unique.|
|Summary   |	Short Summary, external links or references are allowed. |
|Keywords  | A list of keywords that helps others to find the dataset. One of those should be “TPChange (CRC/TRR 301)”. |
|Authors   |	The main researchers involved in producing the data, or the authors of a data publication. |
|Investigator | 	Responsible contact person for the data in question. |
|Metadata 	|Person responsible for the metadata if different from the investigator. |
|Data Description |	The data format as well as additional information about the data if none of the formats netCDF or GRIB are used.|
|License |	Mandatory for external data. Includes information of whether a dataset may be (re-) shared or made available to the public.|

 Optional Information include temporal and spatial coverage as well as the quality control procedure. 


# 4. Data storage, backup and preservation

## 4.1 Data storage
Data storage requirements differ significantly between various projects. While experimental projects usually have data amounts 
on the order of several giga-byte, model based projects (inlcuding projects relying on reanalysis data) can have total data amounts 
on the order of tera-bytes. Especially, for model projects the requirements for disk space easily exceed the basic capacities of 
available data space. Therefore, a diverse strategy is used to cover all needs:

1. Individual storage
When possible local disk space is used for data storage, this ususally applies for small data projects and storage is on local personal 
computers or compute servers. 

2. Disk space on high performance compute clusters
Several high performance computing (HPC) systems are used within TPChange. While several projects use individual computing possibilities 
at DKRZ, JSC, or at GUF, a central project is available on the HPC system of JGU. This central project is funded through basic funding. 

3. Compute and storage server
In Phase 1 a central compute and storage server has been purchased and put into operation at JGU. The server called tpcdata.physic.uni-mainz.de 
is accessible via SSH-methods for members of TPChange, also from remote, i.e., non-JGU, sites. The intention is to allow for relatively simple data exchange between various 
projects, in particular, between model and experimental groups. 

No confidential personal data are collected. Despite this, the following risks exist in the handling of data: 

a) Loss of data and scientific results due to hardware failure or accidentally deletion.  
b) Violation of license agreements by publication of external data or source code.

Hardware failures are accounted for by usage of redundant components and backups. The backup possibilities are presented in 
subsection 4.2. Accidental deletion shall be avoided by appropriate access permission. 


## 4.2 Backup 

For backups the Integrated Rule-Oriented Data System (iRODS) is available at JGU as well as tape archives at DKRZ. The backup of data on personal computers 
lies in the responsibility of the respective person. Backups shall be conducted regularly. Raw and processed data sets shall be added to iRODS,
code shall be version controlled using the the Gitlab instances which are available to the CRC members.


## 4.3 Preservation

The goal for data management is to become able to reproduce experiments and data analysis.

In case of numerical experiments, we do not strive for bit-wise reproducibility but for a repeatability of experiments. 
The former is almost impossible to achieve as factors such as available computers and compilers,
especially in the future, are out of our control. A model experiment becomes repeatable when the 
following conditions are met: 

a) source code of the model is preserved along with the code of tools used for pre- and post-processing;  
b) input data is preserved; and  
c) all settings relevant for the model run (e.g., name-lists) are preserved.

In case of observational and laboratory data, it is essential to preserve the raw data of the experiment.
For laboratory experiments, which are often reproducible, the design and environmental conditions of the experiment shall be preserved.
For field experiements, environmental conditions shall be preserved and auxiliary data such as time and positional data. 
Experimental data undergoes often a data correction to obtain a final data set (in contrast to the raw measurements). The necessary code
for this correction should be part of any preservation of the final, processed data.

Preservation can on the hand use the local iRODS at JGU where TPChange has a dedicated storage which is accessible from the JGU HPC system 
as well as from any local computer within the JGU network. This means also that this allows for preservation of data from remote sites.

Additionally, data sets in publications shall be published to an extent which allows for reproduction of major published results on a public 
data hub. TPChange has a [community on ZENODO](https://zenodo.org/communities/tpchange/) and members of TPChange are encouraged to publish their data there. 
However, in some cases publication is required elsewhere but is then noted in the "About" section in the community. Code can separately be 
published publicly on the TPChange github page. 

# 5. Data sharing and restrictions

## 5.1 Potential targets for sharing
Data created and collected within the project is of interest for (a) readers of TPChange publications, (b) members of TPChange for collaboration, and (c) for future members of TPChange in a possible second and third phase.

## 5.2 Search methods

Readers of publications will find data by means of persistent identifiers (e.g., DOIs).  DOIs or other persistent identifiers are not used for internal data and code, unless it has been published. Within the project data can be shared via the TPChange server or on the HPC systems (DKRZ, MOGON).

## 5.3 Restrictions

|Data Type |	Restrictions|
| ---------| ---------------|
|External Data |	In general, external data are provided by third-parties and shared by them. We do not re-share their data, unless we have explicit permission to do so and the third-party is not sharing the data (e.g., because it has not been archived). |
|Internal Data |	Data created within TPChange are shared for scientific purposes to make experiments repeatable. This is done when license agreements for (input or experimental) data and code permit it. Unless otherwise required, we recommend data publications under one of the creative common licenses.|
|External Code |	Like data, code is shared by the third-parties who own it. For closed-source code, this usually involves signing a license agreement with the code owner. External code is not re-shared, unless it is open source and the license permits it.|
|Internal Code |	Program libraries or other software projects created within TPChange are made available to the public as GIT repository by using code sharing platforms like github.com and gitlab instances (e.g., at JGU or at DKRZ). To gain maximal visibility, repositories are organized in a group called “tpchange”. In addition, persistent identifiers (e.g., DOIs) are obtained from open access code publishing services like Zenodo. Usage of an open-source-license that ensures reproduction of the copyright noticed is recommended (e.g., MIT14). Code that is not published for its own sake, but in support of a publication (e.g., scripts for plots or Jupyter Notebooks), can be published without a repository only on a platform like Zenodo or as supplementary material at the journal. Scientists are encouraged to do so, but without a strict requirement.|

As outlined above, restrictions are required wherever license agreements for data produced by external partners are in place. During the process of collecting data from third-parties, we will already ask for permission to share derived results to minimize restrictions.

In general external data and code are not exclusively available to TPChange. For internally create data and code exclusive access is only required before the publication of scientific results. For all publications regardless of the form, a clear attribution to the project should be possible.

# 6. Responsibilities and resources

## 6.1 Assignment of Responsibilities

The data management plan is created by the Z2 project and reviewed by the steering group. Revised versions are created in the same way. Responsibility for the technical implementation lies with the Z2 members.
 
| Activity |	Responsibility|
| ---------| -----------------|
|Data Collection|  	 |
| *  External Data |	In general, data is centrally collected by Z2 staff. Data collected by project members is included and checked by Z2 staff before sharing within the project.|
| * Internal Data  | Data will be collected by project scientists. Where necessary, with support of Z2 staff. Project scientists are responsible for metadata and documentation.|
| * External Code 	Will be centrally collected by Z2 staff (e.g., ICON and COSMO), but also by project scientists.
| * Internal Code 	Will be collected by project scientists. Where necessary, with support of Z2 staff.
|Metadata 	| |
| * Production |	The same responsibility as for the data itself.|
| * Control  |	Quality control (e.g., completeness) is done by Z2 staff. A technical solution, which automatically checks availability of mandatory information, should be implemented. |
|Data Quality| 	The same responsibility as for the data itself. For external data we rely on quality-control mechanisms of our third-party partners.|
|Storage and Backup| 	The PIs of Z02 coordinate the provision of the actual disk space on the TPChange server. Space on MOGONII/-NHR is provided by the PIs of Z03. Backups of code and data lies in the responsibility of the project PIs and project scientists.|
|Archiving and Sharing |	Individual project scientists with technical support of PIs of Z02.|

It is the responsibility of project PIs to ensure that project scientists respect the policies outlined in this document. PIs can ask for technical support from Z2 staff.

# 7. Ethics and legal compliance

## 7.1 Ethics

No person-related data are collected. Thus, no ethical issues are expected.

## 7.2 Legal compliance

Not all data are owned by the project or project members. To ensure intellectual property rights, the following rules are applied depending on the type of data:

|Type |	Regulation|
| ----| ----------|
|External data |	Data provided by third-parties remain in possession of third-parties and may only be published with their permission. Whether this permission is given is stored as a mandatory attribute of each external dataset.|
|Internal data |	These data are freely reusable within the consortium. For publication a license has to be added. Internal data may not be shared outside of TPChange if it is derived from an external dataset where publication of derived results is not permitted by the data provider.|
|External code |	External source code may only be used in agreement with existing license agreements. Sharing or publication of external source code is only allowed in cases where the license explicitly permits it.|
|Internal code |	Source code produced by members of TPChange is intended for publication along with papers using or describing the code. |

