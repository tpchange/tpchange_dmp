# Data management plan for TPChange

# 1. Administrative data
|   |   |
|---|---|
|Funding Agency	 |    DFG|
|Grant Reference Number	|SFB/TRR 301|
|Project Name	|The Tropopause in a changing atmopshere|
|Project Description	| See <https://tpchange.de> |
|PIs	|  See <https://tpchange.de>|
|Project Data Contact	|Daniel Kunkel  (<dkunkel@uni-mainz.de>)|
|Date of First Version 	|to be filled |
|Date of Last Update	|to be filled |
|Related Policies	| [DFG Leitlinie zum Umgang mit Forschungsdaten](https://www.dfg.de/download/pdf/foerderung/antragstellung/forschungsdaten/richtlinien_forschungsdaten.pdf)|
| |[DFG Leitlinien zur Sicherung guter wissenschaftlicher Praxis](https://www.dfg.de/download/pdf/foerderung/rechtliche_rahmenbedingungen/gute_wissenschaftliche_praxis/kodex_gwp.pdf)|

# 2. Data collection

## 2.1 Sources

TPChange is project at the interface of experimental and theoretical meteoreology. Data within the project covers products from laboratory and field experiments as well as model output of varying complexity. Third-party data from reanalysis data as well as observational data is essential in various proejcts within TPChange. 

Data also comprises program code which is used or which is created which is as essential as the data itself and thus requires to be versionized and eventually published in case of being used for publications. 

To clarify the terminology of data within this plan, the following differentiation is made:

| Type | Description |
|------|-------------|
|External data| Such data is created by third-parties and is available to be used in TPChange. Data sources are known and documented. If possible this data is centrally collected to be easily available for the members of TPChange and to ensure preservation in case the data is used for publications.  |
|Internal data| This data is created by members of TPChange within the projects of TPChange. Such data can comprise output from model experiments as well as results from laboratory and field experiments. In particular, the data from field experiements are crucial data, since they can not easily be reproduced. |
|External code| Program source code provided by third-parties, e.g. code from models such as ICON or MESSy. A second source of such code is proprietary code associated with technical equipment, e.g., for conversion of a measurement signal to atmospherically relevant quantities. A third source of external code is code retrieved from publically available repositories, e.g., GitHub which is used to process data of any kind. |
|Internal code| Source code which is created within the projects of TPChange, independently whether it is model, processing or analysis code.|

## 2.2 Formats

The interchangeability and easy usage of data is achieved through common data types and
formats as well as a comprehensible data structure. Data is preferably stored in the self-
describing format [netCDF](https://www.unidata.ucar.edu/software/netcdf/) where possible since netCDF is well-established within the meteorological community and supported by numerous software tools. Interfaces exist in particular for programming language such as Fortran and C/C++/C# as well as for interpreted languages such as python and julia. These interfaces are open sources and allow to read and write data which in turn allows for long-term accessibility.

Alternatively, text files like software code or data from laboratory and field experiemnts should be used and encoded using UTF8 to ensure portability. Text files from model code and model setups as well as results should be version controlled to make analyses transparent and allow repetition of experiments. 

## 2.3 Structure 

### 2.3.1 Centrally stored data
Within TPChange there are external and internal data sets which are hosted centrally by the projects Z1, Z2, and Z3. This includes data from the field campaign (see project Z01), reanalysis data  (Z02) and model input data (Z03, TO BE CLARIFIED). More so, data is included in this list which is of importance to several projects, e.g., data from the IAGOS database. In detail, centrally stored data is available as follows:

|Data|Location| Original source |Contact |
|---|--| --| --|
|TPChange campaign TPEx | seafile | not yet available | [Heiko Bozem]{mailto:bozemh@uni-mainz.de}, [Peter Hoor]{mailto:hoor@uni-mainz.de} |
|IAGOS (CORE, MOZAIC, CARIBIC)|  mogon      | [http://iagos-data.fr/](http://iagos-data.fr/)|[Daniel Kunkel](mailto:dkunkel@uni-mainz.de)|
| ECMWF ERA5 reanalysis data | mogon | [www.ecmwf.int](https://apps.ecmwf.int/data-catalogues/era5/?class=ea), [EC Wiki](https://confluence.ecmwf.int/display/CKB/ERA5)|  [Daniel Kunkel](mailto:dkunkel@uni-mainz.de)|
| Z01 data product | seafile/tpcdata | various sources | [Hans-Christoph Lachnitt](mailto:lachnit@uni-mainz.de) | 
| MESSy input data | TBC | TBC | TBC |

The centrally stored data will be hosted by the associated contact person. This person is also responsible for meta data availability and potential backup/storage of the data, in particular in case the data is classified as internal data and can not be reproduced from external sources.

**Note:** data will be available on Mogon as well as on the TPChange server. The detailed data availability plan needs still to be implemented.


### 2.3.2 Project specific data
Within the individual projects other internal and external data will be created or used. This data should be described in the porject specific data management plan. More so, the project members are responsible to create meta-data information, versionize data if possible, and store essential data for eventual re-creation of the data.

### 2.3.3 Data availability 

Centrally stored data will be available on MOGON as well as on the TPChange server. The data on the TPChange server is required to be stored in iRODS to allow for quick recovery. The reanalysis data with its more than 200 TB of data is not stored in iRODS but can be recovered from its original source.

## 2.4 Data life cycle

## 2.5 Version control of source code

A central aspect of the data management is to provide a version control software. Such a tool allows to track and provides control over changes to any type of source code which will be created and or used within each project. We will use the state-of-the-art version control system [Git](https://git-scm.org) to establish a central code repository. Git will be used due to its distributed character, its ease of use and the fact that a web-based graphical user interface using Gitlab is already hosted at the Data Centre of the [JGU](https://gitlab.rlp.net). Our Gitlab will be used to share externally developed software, such as the weather model ICON (see also project Z03) as well as all internally developed software code or programme libraries. A common software repository will significantly increase the credibility and reproducibility of scientific results and simplifies re-use of individual members’ developments. Source code can then also be made available to the public using code sharing platforms, e.g., [Github](https://github.com/). The outcome of TPChange will be made available in a group, which further allows to gain maximum visibility. Additionally, persistent identifiers (e.g., digital object identifier - DOI) can be added to the repositories using OpenData services like Zenodo or comparable platforms. 

Zenodo is a publicly available data centre hosted at CERN which provides a DOI along with stored data, thus
making the data citable. More so, the data is directly accessible after publication and also version controlled.
Generally, users, i.e., project partners within the CRC will be supported through training courses on how to best
use Git as well as on documenting and publishing code. The support will be provided through on-site and digital
classes as well as through a communication platform which is linked to the Gitlab instance of the ZDV, also
allowing for interactive digital meetings using BigBlueButton. The ZDV already provides support for Gitlab on
mattermost. Training courses will be held specifically for project members in the first months of the CRC.

Currently, data can is shared and published at the following instances:

|Data type |URL| DOI available | Contact |
|---|--| --| --|

# 3. Data documentation and metadata

## 3.1  Data documentation
Metadata and data documentation are essential when storing data. A self-describing data format will already contain the most important metadata. Furthermore, within the iRODS data archive additional metadata is defined to comprehensively document stored data sets. For data sets from model simulations or laboratory and field experiments the owner is required to add additional information in human readable files, e.g., read-me files in markdown format. The collected information should contain everything needed to repeat any kind of experiments from the initial setup to the final output
following the DFG guidelines on good scientific practice.

## 3.2 Mandatory metadata
The most important metadata are collected within the actual data files using a self-describing format. In addition to that, the CERA2 standard is used for the definition of mandatory information. These metadata are required when datasets are stored within the iRODS archive:  

|Attribute |	Description |
| ---------| -------------- |
|Entry Name| 	A name that ensures unique identification. The combination of Project + Experiment + Dataset-Group must be unique, individual parts may not be unique.|
|Summary   |	Short Summary, external links or references are allowed. |
|Keywords  | A list of keywords that helps others to find the dataset. One of those should be “TPChange (CRC/TRR 301)”. |
|Authors   |	The main researchers involved in producing the data, or the authors of a data publication. |
|Investigator | 	Responsible contact person for the data in question. |
|Metadata 	|Person responsible for the metadata if different from the investigator. |
|Data Description |	The data format as well as additional information about the data if none of the formats netCDF or GRIB are used.|
|License |	Mandatory for external data. Includes information of whether a dataset may be (re-) shared or made available to the public.|

 Optional Information include temporal and spatial coverage as well as the quality control procedure. 


# 4. Data storage, backup and preservation

# 5. Data sharing and restrictions

## 5.1 Potential targets for sharing
Data created and collected within the project is of interest for (a) readers of TPChange publications, (b) members of TPChange for collaboration, and (c) for future members of TPChange in a possible second and third phase.

## 5.2 Search methods

Readers of publications will find data by means of persistent identifiers (e.g., DOIs).  DOIs or other persistent identifiers are not used for internal data and code, unless it has been published. Within the project data can be shared via the TPChange server or on the HPC systems (DKRZ, MOGON).

## 5.3 Restrictions

|Data Type |	Restrictions|
| ---------| ---------------|
|External Data |	In general, external data are provided by third-parties and shared by them. We do not re-share their data, unless we have explicit permission to do so and the third-party is not sharing the data (e.g., because it has not been archived). |
|Internal Data |	Data created within TPChange are shared for scientific purposes to make experiments repeatable. This is done when license agreements for (input or experimental) data and code permit it. Unless otherwise required, we recommend data publications under one of the creative common licenses.|
|External Code |	Like data, code is shared by the third-parties who own it. For closed-source code, this usually involves signing a license agreement with the code owner. External code is not re-shared, unless it is open source and the license permits it.|
|Internal Code |	Program libraries or other software projects created within TPChange are made available to the public as GIT repository by using code sharing platforms like github.com and gitlab instances (e.g., at JGU or at DKRZ). To gain maximal visibility, repositories are organized in a group called “tpchange”. In addition, persistent identifiers (e.g., DOIs) are obtained from open access code publishing services like Zenodo. Usage of an open-source-license that ensures reproduction of the copyright noticed is recommended (e.g., MIT14). Code that is not published for its own sake, but in support of a publication (e.g., scripts for plots or Jupyter Notebooks), can be published without a repository only on a platform like Zenodo or as supplementary material at the journal. Scientists are encouraged to do so, but without a strict requirement.|

As outlined above, restrictions are required wherever license agreements for data produced by external partners are in place. During the process of collecting data from third-parties, we will already ask for permission to share derived results to minimize restrictions.

In general external data and code are not exclusively available to TPChange. For internally create data and code exclusive access is only required before the publication of scientific results. For all publications regardless of the form, a clear attribution to the project should be possible.

# 6. Responsibilities and resources

## 6.1 Assignment of Responsibilities

The data management plan is created by the Z2 project and reviewed by the steering group. Revised versions are created in the same way. Responsibility for the technical implementation lies with the Z2 members.
 
| Activity |	Responsibility|
| ---------| -----------------|
|Data Collection|  	 |
| *  External Data |	In general, data is centrally collected by Z2 staff. Data collected by project members is included and checked by Z2 staff before sharing within the project.|
| * Internal Data  | Data will be collected by project scientists. Where necessary, with support of Z2 staff. Project scientists are responsible for metadata and documentation.|
| * External Code 	Will be centrally collected by Z2 staff (e.g., ICON and COSMO), but also by project scientists.
| * Internal Code 	Will be collected by project scientists. Where necessary, with support of Z2 staff.
|Metadata 	| |
| * Production |	The same responsibility as for the data itself.|
| * Control  |	Quality control (e.g., completeness) is done by Z2 staff. A technical solution, which automatically checks availability of mandatory information, should be implemented. |
|Data Quality| 	The same responsibility as for the data itself. For external data we rely on quality-control mechanisms of our third-party partners.|
|Storage and Backup| 	The PIs of Z02 coordinate the provision of the actual disk space on the TPChange server. Space on MOGONII/-NHR is provided by the PIs of Z03. Backups of code and data lies in the responsibility of the project PIs and project scientists.|
|Archiving and Sharing |	Individual project scientists with technical support of PIs of Z02.|

It is the responsibility of project PIs to ensure that project scientists respect the policies outlined in this document. PIs can ask for technical support from Z2 staff.

# 7. Ethics and legal compliance

## 7.1 Ethics

No person-related data are collected. Thus, no ethical issues are expected.

## 7.2 Legal compliance

Not all data are owned by the project or project members. To ensure intellectual property rights, the following rules are applied depending on the type of data:

|Type |	Regulation|
| ----| ----------|
|External data |	Data provided by third-parties remain in possession of third-parties and may only be published with their permission. Whether this permission is given is stored as a mandatory attribute of each external dataset.|
|Internal data |	These data are freely reusable within the consortium. For publication a license has to be added. Internal data may not be shared outside of TPChange if it is derived from an external dataset where publication of derived results is not permitted by the data provider.|
|External code |	External source code may only be used in agreement with existing license agreements. Sharing or publication of external source code is only allowed in cases where the license explicitly permits it.|
|Internal code |	Source code produced by members of TPChange is intended for publication along with papers using or describing the code. |

